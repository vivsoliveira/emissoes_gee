{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d367cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT = Dados por município 12.0\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import traceback\n",
    "import sys\n",
    "\n",
    "# Caminhos relativos ao projeto\n",
    "ROOT = Path(\"Dados por município 12.0\")\n",
    "OUTPUT_PARQUET = Path(\"todas_emissoes.parquet\")\n",
    "print(\"ROOT =\", ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9893ade0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_lfs_pointer(path: Path, nlines: int = 5) -> bool:\n",
    "    \"\"\"Verifica se o arquivo começa com o ponteiro Git LFS.\"\"\"\n",
    "    try:\n",
    "        with path.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            head = \"\".join([next(f) for _ in range(nlines)])\n",
    "        return \"git-lfs.github.com/spec/v1\" in head or head.strip().startswith(\"version https://git-lfs.github.com/spec/v1\")\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def read_csv_flexible(path: Path, encodings=None, seps=None, chunksize=200_000):\n",
    "    \"\"\"Tenta ler um CSV com encodings e separadores comuns; usa chunks para arquivos grandes.\"\"\"\n",
    "    if encodings is None:\n",
    "        encodings = [\"utf-8\", \"latin1\", \"iso-8859-1\"]\n",
    "    if seps is None:\n",
    "        seps = [\",\", \";\", \"\\t\"]\n",
    "\n",
    "    # tenta leituras de amostra para descobrir separador/encoding\n",
    "    for enc in encodings:\n",
    "        for sep in seps:\n",
    "            try:\n",
    "                # ler uma amostra pequena para validar\n",
    "                pd.read_csv(path, encoding=enc, sep=sep, nrows=5)\n",
    "                # se não deu exceção, procede a leitura completa (por chunks se muito grande)\n",
    "                filesize_mb = path.stat().st_size / (1024 * 1024)\n",
    "                if filesize_mb > 50:  # ajuste threshold se quiser\n",
    "                    print(f\"  Lendo por chunks: {path.name} ({filesize_mb:.1f} MB) encoding={enc} sep={repr(sep)}\")\n",
    "                    parts = []\n",
    "                    for chunk in pd.read_csv(path, encoding=enc, sep=sep, chunksize=chunksize):\n",
    "                        parts.append(chunk)\n",
    "                    return pd.concat(parts, ignore_index=True)\n",
    "                else:\n",
    "                    print(f\"  Lendo arquivo: {path.name} encoding={enc} sep={repr(sep)}\")\n",
    "                    return pd.read_csv(path, encoding=enc, sep=sep)\n",
    "            except Exception:\n",
    "                continue\n",
    "    # se chegou aqui, nenhuma tentativa funcionou\n",
    "    raise ValueError(f\"Não foi possível ler {path} com encodings/separadores testados.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca8d1309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encontrei 44 arquivos CSV (recursivamente).\n",
      "[1/44] Processando: Dados por município 12.0\\AC\\gases.csv\n",
      "  Lendo arquivo: gases.csv encoding=utf-8 sep=','\n",
      "  -> OK: 8944 linhas lidas.\n",
      "[2/44] Processando: Dados por município 12.0\\AM\\ar4.csv\n",
      "  Lendo arquivo: ar4.csv encoding=utf-8 sep=','\n",
      "  -> OK: 8944 linhas lidas.\n",
      "[2/44] Processando: Dados por município 12.0\\AM\\ar4.csv\n",
      "  Lendo arquivo: ar4.csv encoding=utf-8 sep=','\n",
      "  -> OK: 29078 linhas lidas.\n",
      "[3/44] Processando: Dados por município 12.0\\AM\\ar6.csv\n",
      "  Lendo arquivo: ar6.csv encoding=utf-8 sep=','\n",
      "  -> OK: 29078 linhas lidas.\n",
      "[4/44] Processando: Dados por município 12.0\\AP\\ar2.csv\n",
      "  Lendo arquivo: ar2.csv encoding=utf-8 sep=','\n",
      "  -> OK: 29078 linhas lidas.\n",
      "[3/44] Processando: Dados por município 12.0\\AM\\ar6.csv\n",
      "  Lendo arquivo: ar6.csv encoding=utf-8 sep=','\n",
      "  -> OK: 29078 linhas lidas.\n",
      "[4/44] Processando: Dados por município 12.0\\AP\\ar2.csv\n",
      "  Lendo arquivo: ar2.csv encoding=utf-8 sep=','\n",
      "  -> OK: 8516 linhas lidas.\n",
      "[5/44] Processando: Dados por município 12.0\\AP\\ar4.csv\n",
      "  Lendo arquivo: ar4.csv encoding=utf-8 sep=','\n",
      "  -> OK: 8516 linhas lidas.\n",
      "[6/44] Processando: Dados por município 12.0\\AP\\ar5.csv\n",
      "  Lendo arquivo: ar5.csv encoding=utf-8 sep=','\n",
      "  -> OK: 8516 linhas lidas.\n",
      "[7/44] Processando: Dados por município 12.0\\AP\\ar6.csv\n",
      "  Lendo arquivo: ar6.csv encoding=utf-8 sep=','\n",
      "  -> OK: 8516 linhas lidas.\n",
      "[8/44] Processando: Dados por município 12.0\\AP\\gases.csv\n",
      "  Lendo arquivo: gases.csv encoding=utf-8 sep=','\n",
      "  -> OK: 8516 linhas lidas.\n",
      "[5/44] Processando: Dados por município 12.0\\AP\\ar4.csv\n",
      "  Lendo arquivo: ar4.csv encoding=utf-8 sep=','\n",
      "  -> OK: 8516 linhas lidas.\n",
      "[6/44] Processando: Dados por município 12.0\\AP\\ar5.csv\n",
      "  Lendo arquivo: ar5.csv encoding=utf-8 sep=','\n",
      "  -> OK: 8516 linhas lidas.\n",
      "[7/44] Processando: Dados por município 12.0\\AP\\ar6.csv\n",
      "  Lendo arquivo: ar6.csv encoding=utf-8 sep=','\n",
      "  -> OK: 8516 linhas lidas.\n",
      "[8/44] Processando: Dados por município 12.0\\AP\\gases.csv\n",
      "  Lendo arquivo: gases.csv encoding=utf-8 sep=','\n",
      "  -> OK: 7466 linhas lidas.\n",
      "[9/44] Processando: Dados por município 12.0\\CE\\ar5.csv\n",
      "  Lendo por chunks: ar5.csv (54.6 MB) encoding=utf-8 sep=','\n",
      "  -> OK: 7466 linhas lidas.\n",
      "[9/44] Processando: Dados por município 12.0\\CE\\ar5.csv\n",
      "  Lendo por chunks: ar5.csv (54.6 MB) encoding=utf-8 sep=','\n",
      "  -> OK: 80488 linhas lidas.\n",
      "[10/44] Processando: Dados por município 12.0\\DF\\ar2.csv\n",
      "  Lendo arquivo: ar2.csv encoding=utf-8 sep=','\n",
      "  -> OK: 1350 linhas lidas.\n",
      "[11/44] Processando: Dados por município 12.0\\DF\\ar4.csv\n",
      "  Lendo arquivo: ar4.csv encoding=utf-8 sep=','\n",
      "  -> OK: 1350 linhas lidas.\n",
      "[12/44] Processando: Dados por município 12.0\\DF\\ar5.csv\n",
      "  Lendo arquivo: ar5.csv encoding=utf-8 sep=','\n",
      "  -> OK: 1350 linhas lidas.\n",
      "[13/44] Processando: Dados por município 12.0\\DF\\gases.csv\n",
      "  Lendo arquivo: gases.csv encoding=utf-8 sep=','\n",
      "  -> OK: 1542 linhas lidas.\n",
      "[14/44] Processando: Dados por município 12.0\\ES\\ar2.csv\n",
      "  Lendo arquivo: ar2.csv encoding=utf-8 sep=','\n",
      "  -> OK: 80488 linhas lidas.\n",
      "[10/44] Processando: Dados por município 12.0\\DF\\ar2.csv\n",
      "  Lendo arquivo: ar2.csv encoding=utf-8 sep=','\n",
      "  -> OK: 1350 linhas lidas.\n",
      "[11/44] Processando: Dados por município 12.0\\DF\\ar4.csv\n",
      "  Lendo arquivo: ar4.csv encoding=utf-8 sep=','\n",
      "  -> OK: 1350 linhas lidas.\n",
      "[12/44] Processando: Dados por município 12.0\\DF\\ar5.csv\n",
      "  Lendo arquivo: ar5.csv encoding=utf-8 sep=','\n",
      "  -> OK: 1350 linhas lidas.\n",
      "[13/44] Processando: Dados por município 12.0\\DF\\gases.csv\n",
      "  Lendo arquivo: gases.csv encoding=utf-8 sep=','\n",
      "  -> OK: 1542 linhas lidas.\n",
      "[14/44] Processando: Dados por município 12.0\\ES\\ar2.csv\n",
      "  Lendo arquivo: ar2.csv encoding=utf-8 sep=','\n",
      "  -> OK: 35578 linhas lidas.\n",
      "[15/44] Processando: Dados por município 12.0\\ES\\ar6.csv\n",
      "  Lendo arquivo: ar6.csv encoding=utf-8 sep=','\n",
      "  -> OK: 35578 linhas lidas.\n",
      "[15/44] Processando: Dados por município 12.0\\ES\\ar6.csv\n",
      "  Lendo arquivo: ar6.csv encoding=utf-8 sep=','\n",
      "  -> OK: 35578 linhas lidas.\n",
      "[16/44] Processando: Dados por município 12.0\\GO\\ar2.csv\n",
      "  Lendo por chunks: ar2.csv (90.1 MB) encoding=utf-8 sep=','\n",
      "  -> OK: 35578 linhas lidas.\n",
      "[16/44] Processando: Dados por município 12.0\\GO\\ar2.csv\n",
      "  Lendo por chunks: ar2.csv (90.1 MB) encoding=utf-8 sep=','\n",
      "  -> OK: 136852 linhas lidas.\n",
      "[17/44] Processando: Dados por município 12.0\\GO\\ar4.csv\n",
      "  Lendo por chunks: ar4.csv (90.2 MB) encoding=utf-8 sep=','\n",
      "  -> OK: 136852 linhas lidas.\n",
      "[17/44] Processando: Dados por município 12.0\\GO\\ar4.csv\n",
      "  Lendo por chunks: ar4.csv (90.2 MB) encoding=utf-8 sep=','\n",
      "  -> OK: 136852 linhas lidas.\n",
      "[18/44] Processando: Dados por município 12.0\\GO\\ar5.csv\n",
      "  Lendo por chunks: ar5.csv (90.2 MB) encoding=utf-8 sep=','\n",
      "  -> OK: 136852 linhas lidas.\n",
      "[18/44] Processando: Dados por município 12.0\\GO\\ar5.csv\n",
      "  Lendo por chunks: ar5.csv (90.2 MB) encoding=utf-8 sep=','\n",
      "  -> OK: 136852 linhas lidas.\n",
      "[19/44] Processando: Dados por município 12.0\\MT\\ar2.csv\n",
      "  Lendo por chunks: ar2.csv (74.2 MB) encoding=utf-8 sep=','\n",
      "  -> OK: 136852 linhas lidas.\n",
      "[19/44] Processando: Dados por município 12.0\\MT\\ar2.csv\n",
      "  Lendo por chunks: ar2.csv (74.2 MB) encoding=utf-8 sep=','\n",
      "  -> OK: 116714 linhas lidas.\n",
      "[20/44] Processando: Dados por município 12.0\\MT\\ar5.csv\n",
      "  Lendo por chunks: ar5.csv (74.3 MB) encoding=utf-8 sep=','\n",
      "  -> OK: 116714 linhas lidas.\n",
      "[20/44] Processando: Dados por município 12.0\\MT\\ar5.csv\n",
      "  Lendo por chunks: ar5.csv (74.3 MB) encoding=utf-8 sep=','\n",
      "  -> OK: 116714 linhas lidas.\n",
      "[21/44] Processando: Dados por município 12.0\\MT\\ar6.csv\n",
      "  Lendo por chunks: ar6.csv (74.4 MB) encoding=utf-8 sep=','\n",
      "  -> OK: 116714 linhas lidas.\n",
      "[21/44] Processando: Dados por município 12.0\\MT\\ar6.csv\n",
      "  Lendo por chunks: ar6.csv (74.4 MB) encoding=utf-8 sep=','\n",
      "  -> OK: 116714 linhas lidas.\n",
      "[22/44] Processando: Dados por município 12.0\\MT\\gases.csv\n",
      "  Lendo por chunks: gases.csv (63.0 MB) encoding=utf-8 sep=','\n",
      "  -> OK: 116714 linhas lidas.\n",
      "[22/44] Processando: Dados por município 12.0\\MT\\gases.csv\n",
      "  Lendo por chunks: gases.csv (63.0 MB) encoding=utf-8 sep=','\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vitoria Oliveira\\AppData\\Local\\Temp\\ipykernel_19996\\2584397836.py:28: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(path, encoding=enc, sep=sep, chunksize=chunksize):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> OK: 101399 linhas lidas.\n",
      "[23/44] Processando: Dados por município 12.0\\NA\\ar5.csv\n",
      "  Lendo arquivo: ar5.csv encoding=utf-8 sep=','\n",
      "  -> OK: 952 linhas lidas.\n",
      "[24/44] Processando: Dados por município 12.0\\NA\\ar6.csv\n",
      "  Lendo arquivo: ar6.csv encoding=utf-8 sep=','\n",
      "  -> OK: 951 linhas lidas.\n",
      "[25/44] Processando: Dados por município 12.0\\PA\\ar5.csv\n",
      "  Lendo arquivo: ar5.csv encoding=utf-8 sep=','\n",
      "  -> OK: 75380 linhas lidas.\n",
      "[26/44] Processando: Dados por município 12.0\\PA\\ar6.csv\n",
      "  Lendo arquivo: ar6.csv encoding=utf-8 sep=','\n",
      "  -> OK: 75380 linhas lidas.\n",
      "[26/44] Processando: Dados por município 12.0\\PA\\ar6.csv\n",
      "  Lendo arquivo: ar6.csv encoding=utf-8 sep=','\n",
      "  -> OK: 75380 linhas lidas.\n",
      "[27/44] Processando: Dados por município 12.0\\PA\\gases.csv\n",
      "  Lendo arquivo: gases.csv encoding=utf-8 sep=','\n",
      "  -> OK: 75380 linhas lidas.\n",
      "[27/44] Processando: Dados por município 12.0\\PA\\gases.csv\n",
      "  Lendo arquivo: gases.csv encoding=utf-8 sep=','\n",
      "  -> OK: 64719 linhas lidas.\n",
      "[28/44] Processando: Dados por município 12.0\\PE\\ar4.csv\n",
      "  Lendo por chunks: ar4.csv (60.4 MB) encoding=utf-8 sep=','\n",
      "  -> OK: 64719 linhas lidas.\n",
      "[28/44] Processando: Dados por município 12.0\\PE\\ar4.csv\n",
      "  Lendo por chunks: ar4.csv (60.4 MB) encoding=utf-8 sep=','\n",
      "  -> OK: 83526 linhas lidas.\n",
      "[29/44] Processando: Dados por município 12.0\\PE\\ar5.csv\n",
      "  Lendo por chunks: ar5.csv (60.4 MB) encoding=utf-8 sep=','\n",
      "  -> OK: 83526 linhas lidas.\n",
      "[29/44] Processando: Dados por município 12.0\\PE\\ar5.csv\n",
      "  Lendo por chunks: ar5.csv (60.4 MB) encoding=utf-8 sep=','\n",
      "  -> OK: 83526 linhas lidas.\n",
      "[30/44] Processando: Dados por município 12.0\\PI\\ar5.csv\n",
      "  Lendo por chunks: ar5.csv (70.3 MB) encoding=utf-8 sep=','\n",
      "  -> OK: 83526 linhas lidas.\n",
      "[30/44] Processando: Dados por município 12.0\\PI\\ar5.csv\n",
      "  Lendo por chunks: ar5.csv (70.3 MB) encoding=utf-8 sep=','\n",
      "  -> OK: 104944 linhas lidas.\n",
      "[31/44] Processando: Dados por município 12.0\\PR\\ar5.csv\n",
      "  Lendo por chunks: ar5.csv (124.9 MB) encoding=utf-8 sep=','\n",
      "  -> OK: 104944 linhas lidas.\n",
      "[31/44] Processando: Dados por município 12.0\\PR\\ar5.csv\n",
      "  Lendo por chunks: ar5.csv (124.9 MB) encoding=utf-8 sep=','\n",
      "  -> OK: 180598 linhas lidas.\n",
      "[32/44] Processando: Dados por município 12.0\\RO\\ar5.csv\n",
      "  Lendo arquivo: ar5.csv encoding=utf-8 sep=','\n",
      "  -> OK: 26052 linhas lidas.\n",
      "[33/44] Processando: Dados por município 12.0\\RR\\ar2.csv\n",
      "  Lendo arquivo: ar2.csv encoding=utf-8 sep=','\n",
      "  -> OK: 180598 linhas lidas.\n",
      "[32/44] Processando: Dados por município 12.0\\RO\\ar5.csv\n",
      "  Lendo arquivo: ar5.csv encoding=utf-8 sep=','\n",
      "  -> OK: 26052 linhas lidas.\n",
      "[33/44] Processando: Dados por município 12.0\\RR\\ar2.csv\n",
      "  Lendo arquivo: ar2.csv encoding=utf-8 sep=','\n",
      "  -> OK: 8588 linhas lidas.\n",
      "[34/44] Processando: Dados por município 12.0\\RR\\ar4.csv\n",
      "  Lendo arquivo: ar4.csv encoding=utf-8 sep=','\n",
      "  -> OK: 8588 linhas lidas.\n",
      "[35/44] Processando: Dados por município 12.0\\RR\\ar5.csv\n",
      "  Lendo arquivo: ar5.csv encoding=utf-8 sep=','\n",
      "  -> OK: 8588 linhas lidas.\n",
      "[36/44] Processando: Dados por município 12.0\\RR\\gases.csv\n",
      "  Lendo arquivo: gases.csv encoding=utf-8 sep=','\n",
      "  -> OK: 7291 linhas lidas.\n",
      "[37/44] Processando: Dados por município 12.0\\RS\\ar5.csv\n",
      "  -> OK: 8588 linhas lidas.\n",
      "[34/44] Processando: Dados por município 12.0\\RR\\ar4.csv\n",
      "  Lendo arquivo: ar4.csv encoding=utf-8 sep=','\n",
      "  -> OK: 8588 linhas lidas.\n",
      "[35/44] Processando: Dados por município 12.0\\RR\\ar5.csv\n",
      "  Lendo arquivo: ar5.csv encoding=utf-8 sep=','\n",
      "  -> OK: 8588 linhas lidas.\n",
      "[36/44] Processando: Dados por município 12.0\\RR\\gases.csv\n",
      "  Lendo arquivo: gases.csv encoding=utf-8 sep=','\n",
      "  -> OK: 7291 linhas lidas.\n",
      "[37/44] Processando: Dados por município 12.0\\RS\\ar5.csv\n",
      "  Lendo por chunks: ar5.csv (185.1 MB) encoding=utf-8 sep=','\n",
      "  Lendo por chunks: ar5.csv (185.1 MB) encoding=utf-8 sep=','\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vitoria Oliveira\\AppData\\Local\\Temp\\ipykernel_19996\\2584397836.py:28: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(path, encoding=enc, sep=sep, chunksize=chunksize):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> OK: 275006 linhas lidas.\n",
      "[38/44] Processando: Dados por município 12.0\\RS\\ar6.csv\n",
      "  Lendo por chunks: ar6.csv (185.4 MB) encoding=utf-8 sep=','\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vitoria Oliveira\\AppData\\Local\\Temp\\ipykernel_19996\\2584397836.py:28: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(path, encoding=enc, sep=sep, chunksize=chunksize):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> OK: 275006 linhas lidas.\n",
      "[39/44] Processando: Dados por município 12.0\\SP\\ar2.csv\n",
      "  Lendo por chunks: ar2.csv (265.4 MB) encoding=utf-8 sep=','\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vitoria Oliveira\\AppData\\Local\\Temp\\ipykernel_19996\\2584397836.py:28: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(path, encoding=enc, sep=sep, chunksize=chunksize):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> OK: 384790 linhas lidas.\n",
      "[40/44] Processando: Dados por município 12.0\\SP\\gases.csv\n",
      "  Lendo por chunks: gases.csv (222.1 MB) encoding=utf-8 sep=','\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vitoria Oliveira\\AppData\\Local\\Temp\\ipykernel_19996\\2584397836.py:28: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(path, encoding=enc, sep=sep, chunksize=chunksize):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> OK: 341661 linhas lidas.\n",
      "[41/44] Processando: Dados por município 12.0\\TO\\ar4.csv\n",
      "  Lendo por chunks: ar4.csv (50.1 MB) encoding=utf-8 sep=','\n",
      "  -> OK: 79436 linhas lidas.\n",
      "[42/44] Processando: Dados por município 12.0\\TO\\ar5.csv\n",
      "  Lendo por chunks: ar5.csv (50.1 MB) encoding=utf-8 sep=','\n",
      "  -> OK: 79436 linhas lidas.\n",
      "[42/44] Processando: Dados por município 12.0\\TO\\ar5.csv\n",
      "  Lendo por chunks: ar5.csv (50.1 MB) encoding=utf-8 sep=','\n",
      "  -> OK: 79436 linhas lidas.\n",
      "[43/44] Processando: Dados por município 12.0\\TO\\ar6.csv\n",
      "  Lendo por chunks: ar6.csv (50.2 MB) encoding=utf-8 sep=','\n",
      "  -> OK: 79436 linhas lidas.\n",
      "[43/44] Processando: Dados por município 12.0\\TO\\ar6.csv\n",
      "  Lendo por chunks: ar6.csv (50.2 MB) encoding=utf-8 sep=','\n",
      "  -> OK: 79436 linhas lidas.\n",
      "[44/44] Processando: Dados por município 12.0\\TO\\gases.csv\n",
      "  Lendo arquivo: gases.csv encoding=utf-8 sep=','\n",
      "  -> OK: 79436 linhas lidas.\n",
      "[44/44] Processando: Dados por município 12.0\\TO\\gases.csv\n",
      "  Lendo arquivo: gases.csv encoding=utf-8 sep=','\n",
      "  -> OK: 62572 linhas lidas.\n",
      "\n",
      "Resumo:\n",
      "Arquivos LFS pendentes: 0\n",
      "Erros de leitura: 0\n",
      "DataFrames lidos: 44\n",
      "  -> OK: 62572 linhas lidas.\n",
      "\n",
      "Resumo:\n",
      "Arquivos LFS pendentes: 0\n",
      "Erros de leitura: 0\n",
      "DataFrames lidos: 44\n"
     ]
    }
   ],
   "source": [
    "if not ROOT.exists():\n",
    "    raise FileNotFoundError(f\"Pasta root não encontrada: {ROOT.resolve()}\")\n",
    "\n",
    "arquivos = list(ROOT.rglob(\"*.csv\"))\n",
    "print(f\"Encontrei {len(arquivos)} arquivos CSV (recursivamente).\")\n",
    "\n",
    "dfs = []\n",
    "arquivos_lfs = []\n",
    "erros = []\n",
    "\n",
    "for i, arq in enumerate(arquivos, 1):\n",
    "    print(f\"[{i}/{len(arquivos)}] Processando: {arq}\")\n",
    "    try:\n",
    "        if is_lfs_pointer(arq):\n",
    "            arquivos_lfs.append(arq)\n",
    "            print(\"  -> PARECE ponteiro LFS (ainda não baixado). Rode `git lfs pull` se necessário.\")\n",
    "            continue\n",
    "        df_temp = read_csv_flexible(arq)\n",
    "        # opcional: adicionar coluna com origem para rastreabilidade\n",
    "        df_temp[\"_origem_arquivo\"] = str(arq.relative_to(ROOT))\n",
    "        dfs.append(df_temp)\n",
    "        print(f\"  -> OK: {len(df_temp)} linhas lidas.\")\n",
    "    except Exception as e:\n",
    "        tb = traceback.format_exc()\n",
    "        erros.append((str(arq), str(e)))\n",
    "        print(f\"  -> ERRO ao ler {arq.name}: {e}\")\n",
    "\n",
    "print(\"\\nResumo:\")\n",
    "print(\"Arquivos LFS pendentes:\", len(arquivos_lfs))\n",
    "print(\"Erros de leitura:\", len(erros))\n",
    "print(\"DataFrames lidos:\", len(dfs))\n",
    "\n",
    "# listar alguns exemplos para inspeção\n",
    "if arquivos_lfs:\n",
    "    print(\"\\nExemplos de arquivos que parecem ponteiros LFS (mostrando até 10):\")\n",
    "    for p in arquivos_lfs[:10]:\n",
    "        print(\" -\", p)\n",
    "\n",
    "if erros:\n",
    "    print(\"\\nExemplos de erros (até 5):\")\n",
    "    for fn, msg in erros[:5]:\n",
    "        print(\" -\", fn, \":\", msg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "232b005b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenei DataFrames. Linhas: 3434389 Colunas: 66\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 28] Error writing bytes to file. Detail: [errno 28] No space left on device",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m df_all \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(dfs, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConcatenei DataFrames. Linhas:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(df_all), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColunas:\u001b[39m\u001b[38;5;124m\"\u001b[39m, df_all\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m----> 6\u001b[0m \u001b[43mdf_all\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mOUTPUT_PARQUET\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSalvo em:\u001b[39m\u001b[38;5;124m\"\u001b[39m, OUTPUT_PARQUET)\n\u001b[0;32m      8\u001b[0m display(df_all\u001b[38;5;241m.\u001b[39mhead())\n",
      "File \u001b[1;32mc:\\Users\\Vitoria Oliveira\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    332\u001b[0m     )\n\u001b[1;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Vitoria Oliveira\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:3113\u001b[0m, in \u001b[0;36mDataFrame.to_parquet\u001b[1;34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001b[0m\n\u001b[0;32m   3032\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3033\u001b[0m \u001b[38;5;124;03mWrite a DataFrame to the binary parquet format.\u001b[39;00m\n\u001b[0;32m   3034\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3109\u001b[0m \u001b[38;5;124;03m>>> content = f.read()\u001b[39;00m\n\u001b[0;32m   3110\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3111\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparquet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_parquet\n\u001b[1;32m-> 3113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3114\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartition_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3121\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3122\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Vitoria Oliveira\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parquet.py:480\u001b[0m, in \u001b[0;36mto_parquet\u001b[1;34m(df, path, engine, compression, index, storage_options, partition_cols, filesystem, **kwargs)\u001b[0m\n\u001b[0;32m    476\u001b[0m impl \u001b[38;5;241m=\u001b[39m get_engine(engine)\n\u001b[0;32m    478\u001b[0m path_or_buf: FilePath \u001b[38;5;241m|\u001b[39m WriteBuffer[\u001b[38;5;28mbytes\u001b[39m] \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO() \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m path\n\u001b[1;32m--> 480\u001b[0m \u001b[43mimpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    481\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartition_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    486\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    492\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, io\u001b[38;5;241m.\u001b[39mBytesIO)\n",
      "File \u001b[1;32mc:\\Users\\Vitoria Oliveira\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parquet.py:228\u001b[0m, in \u001b[0;36mPyArrowImpl.write\u001b[1;34m(self, df, path, compression, index, storage_options, partition_cols, filesystem, **kwargs)\u001b[0m\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mparquet\u001b[38;5;241m.\u001b[39mwrite_to_dataset(\n\u001b[0;32m    219\u001b[0m             table,\n\u001b[0;32m    220\u001b[0m             path_or_handle,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    224\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    225\u001b[0m         )\n\u001b[0;32m    226\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    227\u001b[0m         \u001b[38;5;66;03m# write to single output file\u001b[39;00m\n\u001b[1;32m--> 228\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Vitoria Oliveira\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyarrow\\parquet\\core.py:1983\u001b[0m, in \u001b[0;36mwrite_table\u001b[1;34m(table, where, row_group_size, version, use_dictionary, compression, write_statistics, use_deprecated_int96_timestamps, coerce_timestamps, allow_truncated_timestamps, data_page_size, flavor, filesystem, compression_level, use_byte_stream_split, column_encoding, data_page_version, use_compliant_nested_type, encryption_properties, write_batch_size, dictionary_pagesize_limit, store_schema, write_page_index, write_page_checksum, sorting_columns, store_decimal_as_integer, **kwargs)\u001b[0m\n\u001b[0;32m   1956\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1957\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ParquetWriter(\n\u001b[0;32m   1958\u001b[0m             where, table\u001b[38;5;241m.\u001b[39mschema,\n\u001b[0;32m   1959\u001b[0m             filesystem\u001b[38;5;241m=\u001b[39mfilesystem,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1981\u001b[0m             store_decimal_as_integer\u001b[38;5;241m=\u001b[39mstore_decimal_as_integer,\n\u001b[0;32m   1982\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m writer:\n\u001b[1;32m-> 1983\u001b[0m         \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow_group_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrow_group_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1984\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1985\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path_like(where):\n",
      "File \u001b[1;32mc:\\Users\\Vitoria Oliveira\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyarrow\\parquet\\core.py:1166\u001b[0m, in \u001b[0;36mParquetWriter.write_table\u001b[1;34m(self, table, row_group_size)\u001b[0m\n\u001b[0;32m   1160\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1161\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTable schema does not match schema used to create file: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1162\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtable:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtable\u001b[38;5;241m.\u001b[39mschema\u001b[38;5;132;01m!s}\u001b[39;00m\u001b[38;5;124m vs. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mfile:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema\u001b[38;5;132;01m!s}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1163\u001b[0m     )\n\u001b[0;32m   1164\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m-> 1166\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow_group_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrow_group_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Vitoria Oliveira\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyarrow\\_parquet.pyx:2386\u001b[0m, in \u001b[0;36mpyarrow._parquet.ParquetWriter.write_table\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Vitoria Oliveira\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyarrow\\error.pxi:92\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 28] Error writing bytes to file. Detail: [errno 28] No space left on device"
     ]
    }
   ],
   "source": [
    "if not dfs:\n",
    "    print(\"Nenhum DataFrame válido foi lido. Primeiro resolva os ponteiros LFS com `git lfs pull` e rode novamente.\")\n",
    "else:\n",
    "    df_all = pd.concat(dfs, ignore_index=True)\n",
    "    print(\"Concatenei DataFrames. Linhas:\", len(df_all), \"Colunas:\", df_all.shape[1])\n",
    "    df_all.to_parquet(OUTPUT_PARQUET, index=False)\n",
    "    print(\"Salvo em:\", OUTPUT_PARQUET)\n",
    "    display(df_all.head())\n",
    "    print(\"\\nColunas:\")\n",
    "    print(df_all.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ef6d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Possíveis colunas de município: ['Cidade']\n",
      "Top 10 arquivos por número de linhas:\n",
      "_origem_arquivo\n",
      "SP\\ar2.csv      384790\n",
      "SP\\gases.csv    341661\n",
      "RS\\ar5.csv      275006\n",
      "RS\\ar6.csv      275006\n",
      "PR\\ar5.csv      180598\n",
      "GO\\ar4.csv      136852\n",
      "GO\\ar2.csv      136852\n",
      "GO\\ar5.csv      136852\n",
      "MT\\ar6.csv      116714\n",
      "MT\\ar2.csv      116714\n",
      "Name: count, dtype: int64\n",
      "Colunas parecendo anos: ['1970', '1971', '1972', '1973', '1974', '1975', '1976', '1977', '1978', '1979', '1980', '1981', '1982', '1983', '1984', '1985', '1986', '1987', '1988', '1989']\n"
     ]
    }
   ],
   "source": [
    "# Ajuste estes nomes conforme coluna real do CSV; este é apenas um exemplo útil\n",
    "cols_lower = [c.lower().strip() for c in df_all.columns]\n",
    "possible_city_cols = [c for c in df_all.columns if c.lower().strip() in (\"municipio\",\"cidade\",\"município\",\"nome_municipio\",\"municipio_nome\")]\n",
    "print(\"Possíveis colunas de município:\", possible_city_cols)\n",
    "\n",
    "top_by_origem = df_all[\"_origem_arquivo\"].value_counts().head(10)\n",
    "print(\"Top 10 arquivos por número de linhas:\")\n",
    "print(top_by_origem)\n",
    "\n",
    "year_cols = [c for c in df_all.columns if c.isdigit() and 1800 <= int(c) <= 2100]\n",
    "print(\"Colunas parecendo anos:\", year_cols[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924f4ffb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
