{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93b45112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT: C:\\Users\\Vitoria Oliveira\\Desktop\\bigdata_dp\\aps\\emissoes_gee\\Dados por município 12.0\n",
      "PARTS_OUT: C:\\Users\\Vitoria Oliveira\\Desktop\\bigdata_dp\\aps\\emissoes_gee\\parts_parquet\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import traceback\n",
    "import pyarrow.dataset as ds\n",
    "import numpy as np\n",
    "\n",
    "# ajuste conforme seu caminho\n",
    "ROOT = Path(r\"C:\\Users\\Vitoria Oliveira\\Desktop\\bigdata_dp\\aps\\emissoes_gee\\Dados por município 12.0\")\n",
    "OUT_DIR = Path(r\"C:\\Users\\Vitoria Oliveira\\Desktop\\bigdata_dp\\aps\\emissoes_gee\")\n",
    "PARTS_OUT = OUT_DIR / \"parts_parquet\"\n",
    "FINAL_PARQUET = OUT_DIR / \"emissoes_unidas.parquet\"\n",
    "AGG_DECADA_PARQUET = OUT_DIR / \"emissoes_por_decada.parquet\"\n",
    "\n",
    "PARTS_OUT.mkdir(parents=True, exist_ok=True)\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"ROOT:\", ROOT)\n",
    "print(\"PARTS_OUT:\", PARTS_OUT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "647112f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_lfs_pointer(path: Path, nlines: int = 3) -> bool:\n",
    "    try:\n",
    "        with path.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            head = \"\".join([next(f) for _ in range(nlines)])\n",
    "        return \"git-lfs.github.com/spec/v1\" in head or head.strip().startswith(\"version https://git-lfs.github.com/spec/v1\")\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def try_read_csv(path: Path, encodings=None, seps=None, nrows_sample=5):\n",
    "    \"\"\"Tenta ler o CSV com encodings e separators comuns. Retorna DataFrame ou lança erro.\"\"\"\n",
    "    if encodings is None:\n",
    "        encodings = [\"utf-8\", \"latin1\", \"iso-8859-1\"]\n",
    "    if seps is None:\n",
    "        seps = [\",\", \";\", \"\\t\"]\n",
    "    last_err = None\n",
    "    for enc in encodings:\n",
    "        for sep in seps:\n",
    "            try:\n",
    "                # lê pequena amostra para validar\n",
    "                pd.read_csv(path, encoding=enc, sep=sep, nrows=nrows_sample)\n",
    "                # se ok, faz leitura completa (se muito grande, deixar padrão; se precisar usar chunks, adaptar)\n",
    "                return pd.read_csv(path, encoding=enc, sep=sep)\n",
    "            except Exception as e:\n",
    "                last_err = e\n",
    "                continue\n",
    "    raise ValueError(f\"Não consegui ler {path} com encodings/separadores testados. Último erro: {last_err}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "370eaece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total CSVs encontrados: 44\n",
      "Ponteiros LFS detectados: 0\n",
      "Se houver ponteiros LFS, rode `git lfs pull` na pasta do repositório e execute novamente.\n"
     ]
    }
   ],
   "source": [
    "all_csvs = sorted(ROOT.rglob(\"*.csv\"))\n",
    "print(\"Total CSVs encontrados:\", len(all_csvs))\n",
    "\n",
    "lfs_pointers = [p for p in all_csvs if is_lfs_pointer(p)]\n",
    "print(\"Ponteiros LFS detectados:\", len(lfs_pointers))\n",
    "if lfs_pointers:\n",
    "    print(\"Mostrando até 10 ponteiros LFS:\")\n",
    "    for p in lfs_pointers[:10]:\n",
    "        print(\" -\", p.relative_to(ROOT))\n",
    "print(\"Se houver ponteiros LFS, rode `git lfs pull` na pasta do repositório e execute novamente.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d3cbb46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lendo: AC\\gases.csv\n",
      "Lendo: AM\\ar4.csv\n",
      "Lendo: AM\\ar6.csv\n",
      "Lendo: AP\\ar2.csv\n",
      "Lendo: AP\\ar4.csv\n",
      "Lendo: AP\\ar5.csv\n",
      "Lendo: AP\\ar6.csv\n",
      "Lendo: AP\\gases.csv\n",
      "Lendo: CE\\ar5.csv\n",
      "Lendo: DF\\ar2.csv\n",
      "Lendo: DF\\ar4.csv\n",
      "Lendo: DF\\ar5.csv\n",
      "Lendo: DF\\gases.csv\n",
      "Lendo: ES\\ar2.csv\n",
      "Lendo: ES\\ar6.csv\n",
      "Lendo: GO\\ar2.csv\n",
      "Lendo: GO\\ar4.csv\n",
      "Lendo: GO\\ar5.csv\n",
      "Lendo: MT\\ar2.csv\n",
      "Lendo: MT\\ar5.csv\n",
      "Lendo: MT\\ar6.csv\n",
      "Lendo: MT\\gases.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vitoria Oliveira\\AppData\\Local\\Temp\\ipykernel_12340\\3119464207.py:22: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  return pd.read_csv(path, encoding=enc, sep=sep)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lendo: NA\\ar5.csv\n",
      "Lendo: NA\\ar6.csv\n",
      "Lendo: PA\\ar5.csv\n",
      "Lendo: PA\\ar6.csv\n",
      "Lendo: PA\\gases.csv\n",
      "Lendo: PE\\ar4.csv\n",
      "Lendo: PE\\ar5.csv\n",
      "Lendo: PI\\ar5.csv\n",
      "Lendo: PR\\ar5.csv\n",
      "Lendo: RO\\ar5.csv\n",
      "Lendo: RR\\ar2.csv\n",
      "Lendo: RR\\ar4.csv\n",
      "Lendo: RR\\ar5.csv\n",
      "Lendo: RR\\gases.csv\n",
      "Lendo: RS\\ar5.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vitoria Oliveira\\AppData\\Local\\Temp\\ipykernel_12340\\3119464207.py:22: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  return pd.read_csv(path, encoding=enc, sep=sep)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lendo: RS\\ar6.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vitoria Oliveira\\AppData\\Local\\Temp\\ipykernel_12340\\3119464207.py:22: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  return pd.read_csv(path, encoding=enc, sep=sep)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lendo: SP\\ar2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vitoria Oliveira\\AppData\\Local\\Temp\\ipykernel_12340\\3119464207.py:22: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  return pd.read_csv(path, encoding=enc, sep=sep)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lendo: SP\\gases.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vitoria Oliveira\\AppData\\Local\\Temp\\ipykernel_12340\\3119464207.py:22: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  return pd.read_csv(path, encoding=enc, sep=sep)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lendo: TO\\ar4.csv\n",
      "Lendo: TO\\ar5.csv\n",
      "Lendo: TO\\ar6.csv\n",
      "Lendo: TO\\gases.csv\n",
      "Parts gravadas: 44\n",
      "Erros: 0\n"
     ]
    }
   ],
   "source": [
    "# Leitura file-by-file e gravação em partes parquet (evita estourar disco/ram)\n",
    "parts = []\n",
    "errors = []\n",
    "i = 0\n",
    "\n",
    "for p in all_csvs:\n",
    "    try:\n",
    "        if is_lfs_pointer(p):\n",
    "            print(\"Ponteiro LFS (pular):\", p.relative_to(ROOT))\n",
    "            continue\n",
    "        print(\"Lendo:\", p.relative_to(ROOT))\n",
    "        df = try_read_csv(p)\n",
    "        # garantir nomes limpos\n",
    "        df.columns = [c.strip() for c in df.columns]\n",
    "        # adicionar coluna de origem para rastreabilidade\n",
    "        df[\"_origem_arquivo\"] = str(p.relative_to(ROOT))\n",
    "        part_path = PARTS_OUT / f\"part_{i:04d}.parquet\"\n",
    "        df.to_parquet(part_path, index=False, compression=\"snappy\")\n",
    "        parts.append(part_path)\n",
    "        i += 1\n",
    "    except Exception as e:\n",
    "        errors.append((p, str(e)))\n",
    "        print(\"Erro lendo\", p.relative_to(ROOT), \":\", e)\n",
    "\n",
    "print(\"Parts gravadas:\", len(parts))\n",
    "print(\"Erros:\", len(errors))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20abdc89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parts encontradas: 44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando parts: 100%|██████████| 44/44 [13:12<00:00, 18.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agg parts geradas: 44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "PARTS_OUT = Path(r\"C:\\Users\\Vitoria Oliveira\\Desktop\\bigdata_dp\\aps\\emissoes_gee\\parts_parquet\")\n",
    "AGG_PARTS = PARTS_OUT / \"agg_parts\"\n",
    "AGG_PARTS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def detect_year_cols(cols):\n",
    "    return [c for c in cols if c.isdigit() and 1800 <= int(c) <= 2100]\n",
    "\n",
    "# ajuste o nome do setor caso necessário\n",
    "sector_candidates = [\"Setor de emissão\",\"Setor de Emissão\",\"Setor\",\"Categoria emissora\",\"Categoria Emissora\"]\n",
    "def find_sector_col(cols):\n",
    "    for cand in sector_candidates:\n",
    "        if cand in cols:\n",
    "            return cand\n",
    "    # fallback: tenta encontrar coluna que contenha 'setor' ignorando case\n",
    "    for c in cols:\n",
    "        if 'setor' in c.lower() or 'categoria' in c.lower():\n",
    "            return c\n",
    "    raise KeyError(\"Coluna de setor não encontrada. Colunas: \" + \", \".join(cols))\n",
    "\n",
    "def map_sector(text):\n",
    "    t = str(text).lower()\n",
    "    if \"agro\" in t or \"pecuár\" in t or \"agric\" in t:\n",
    "        return \"Agropecuária\"\n",
    "    if \"energia\" in t or \"combust\" in t or \"bunker\" in t or \"queima\" in t:\n",
    "        return \"Energia e Combustíveis\"\n",
    "    if \"process\" in t or \"industrial\" in t or \"processos\" in t:\n",
    "        return \"Processos Industriais\"\n",
    "    if \"resíduo\" in t or \"residuos\" in t or \"lixo\" in t or \"saneamento\" in t:\n",
    "        return \"Resíduos\"\n",
    "    if \"uso do solo\" in t or \"mudança\" in t or \"desmat\" in t or \"flore\" in t:\n",
    "        return \"Mudança de Uso do Solo\"\n",
    "    return str(text).strip()\n",
    "\n",
    "parts = sorted(PARTS_OUT.glob(\"part_*.parquet\"))\n",
    "print(\"Parts encontradas:\", len(parts))\n",
    "\n",
    "agg_part_files = []\n",
    "for idx, part in enumerate(tqdm(parts, desc=\"Processando parts\")):\n",
    "    try:\n",
    "        df = pd.read_parquet(part)   # parte por parte\n",
    "        df.columns = [c.strip() for c in df.columns]\n",
    "        year_cols = detect_year_cols(df.columns)\n",
    "        if not year_cols:\n",
    "            # se não tiver anos, pula\n",
    "            continue\n",
    "\n",
    "        id_vars = [c for c in df.columns if c not in year_cols]\n",
    "        # melt para long (apenas as colunas desta part)\n",
    "        dlong = df.melt(id_vars=id_vars, value_vars=year_cols, var_name=\"ano\", value_name=\"valor\")\n",
    "        # limpeza e tipos\n",
    "        dlong = dlong.dropna(subset=[\"valor\"])\n",
    "        dlong[\"ano\"] = dlong[\"ano\"].astype(int)\n",
    "        dlong[\"valor\"] = pd.to_numeric(dlong[\"valor\"].astype(str).str.replace(\",\",\".\",regex=False), errors=\"coerce\")\n",
    "        dlong = dlong.dropna(subset=[\"valor\"])\n",
    "\n",
    "        # mapear setor (coluna detectada dinamicamente)\n",
    "        sector_col = find_sector_col(dlong.columns)\n",
    "        dlong[\"Setor_Agrupado\"] = dlong[sector_col].apply(map_sector)\n",
    "\n",
    "        # criar decada\n",
    "        dlong[\"decada\"] = (dlong[\"ano\"] // 10 * 10).astype(int).astype(str) + \"s\"\n",
    "\n",
    "        # escolher colunas para agregação (só as que existem)\n",
    "        agg_by = [\"decada\", \"Setor_Agrupado\"]\n",
    "        if \"Cidade\" in dlong.columns:\n",
    "            agg_by.append(\"Cidade\")\n",
    "        if \"Gas\" in dlong.columns:\n",
    "            agg_by.append(\"Gas\")\n",
    "        else:\n",
    "            # tentar detectar coluna de gás por heurística\n",
    "            gas_cand = [c for c in dlong.columns if \"gás\" in c.lower() or \"gas\"==c.lower()]\n",
    "            if gas_cand:\n",
    "                agg_by.append(gas_cand[0])\n",
    "        if \"Bioma\" in dlong.columns:\n",
    "            agg_by.append(\"Bioma\")\n",
    "\n",
    "        # agregação por esta part\n",
    "        df_agg = dlong.groupby(agg_by, dropna=False)[\"valor\"].sum().reset_index().rename(columns={\"valor\":\"emissao_t\"})\n",
    "\n",
    "        # salvar agregação desta part\n",
    "        outp = AGG_PARTS / f\"agg_part_{idx:04d}.parquet\"\n",
    "        df_agg.to_parquet(outp, index=False, compression=\"snappy\")\n",
    "        agg_part_files.append(outp)\n",
    "    except Exception as e:\n",
    "        print(\"Erro em\", part.name, \":\", e)\n",
    "\n",
    "print(\"Agg parts geradas:\", len(agg_part_files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d364bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivos de agregado a combinar: 44\n",
      "df_decada_final shape: (486495, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>decada</th>\n",
       "      <th>Setor_Agrupado</th>\n",
       "      <th>Cidade</th>\n",
       "      <th>Gás</th>\n",
       "      <th>Bioma</th>\n",
       "      <th>emissao_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1970s</td>\n",
       "      <td>Agropecuária</td>\n",
       "      <td>Abadia de Goiás (GO)</td>\n",
       "      <td>CO2e (t) GTP-AR2</td>\n",
       "      <td>Cerrado</td>\n",
       "      <td>7093.951239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1970s</td>\n",
       "      <td>Agropecuária</td>\n",
       "      <td>Abadia de Goiás (GO)</td>\n",
       "      <td>CO2e (t) GTP-AR4</td>\n",
       "      <td>Cerrado</td>\n",
       "      <td>7093.951239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1970s</td>\n",
       "      <td>Agropecuária</td>\n",
       "      <td>Abadia de Goiás (GO)</td>\n",
       "      <td>CO2e (t) GTP-AR5</td>\n",
       "      <td>Cerrado</td>\n",
       "      <td>6148.091074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1970s</td>\n",
       "      <td>Agropecuária</td>\n",
       "      <td>Abadia de Goiás (GO)</td>\n",
       "      <td>CO2e (t) GWP-AR2</td>\n",
       "      <td>Cerrado</td>\n",
       "      <td>8144.906978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1970s</td>\n",
       "      <td>Agropecuária</td>\n",
       "      <td>Abadia de Goiás (GO)</td>\n",
       "      <td>CO2e (t) GWP-AR4</td>\n",
       "      <td>Cerrado</td>\n",
       "      <td>7829.620257</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  decada Setor_Agrupado                Cidade               Gás    Bioma  \\\n",
       "0  1970s   Agropecuária  Abadia de Goiás (GO)  CO2e (t) GTP-AR2  Cerrado   \n",
       "1  1970s   Agropecuária  Abadia de Goiás (GO)  CO2e (t) GTP-AR4  Cerrado   \n",
       "2  1970s   Agropecuária  Abadia de Goiás (GO)  CO2e (t) GTP-AR5  Cerrado   \n",
       "3  1970s   Agropecuária  Abadia de Goiás (GO)  CO2e (t) GWP-AR2  Cerrado   \n",
       "4  1970s   Agropecuária  Abadia de Goiás (GO)  CO2e (t) GWP-AR4  Cerrado   \n",
       "\n",
       "     emissao_t  \n",
       "0  7093.951239  \n",
       "1  7093.951239  \n",
       "2  6148.091074  \n",
       "3  8144.906978  \n",
       "4  7829.620257  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salvo em: C:\\Users\\Vitoria Oliveira\\Desktop\\bigdata_dp\\aps\\emissoes_gee\\emissoes_por_decada_streaming.parquet\n"
     ]
    }
   ],
   "source": [
    "# Ler todos os agg_parts (muito menores) e somar para obter o agregado final\n",
    "agg_files = sorted((PARTS_OUT / \"agg_parts\").glob(\"agg_part_*.parquet\"))\n",
    "print(\"Arquivos de agregado a combinar:\", len(agg_files))\n",
    "\n",
    "dfs_small = []\n",
    "for f in agg_files:\n",
    "    dfs_small.append(pd.read_parquet(f))\n",
    "\n",
    "if not dfs_small:\n",
    "    raise RuntimeError(\"Nenhum agg_part encontrado — verifique a célula anterior.\")\n",
    "\n",
    "df_agg_all = pd.concat(dfs_small, ignore_index=True)\n",
    "# agrega novamente por garantir soma correta entre partes\n",
    "group_cols = [c for c in df_agg_all.columns if c != \"emissao_t\"]\n",
    "df_decada_final = df_agg_all.groupby(group_cols, dropna=False)[\"emissao_t\"].sum().reset_index()\n",
    "\n",
    "print(\"df_decada_final shape:\", df_decada_final.shape)\n",
    "display(df_decada_final.head())\n",
    "\n",
    "# salvar resultado final\n",
    "out_final = PARTS_OUT.parent / \"emissoes_por_decada_streaming.parquet\"\n",
    "df_decada_final.to_parquet(out_final, index=False, compression=\"snappy\")\n",
    "print(\"Salvo em:\", out_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3e7cf8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
